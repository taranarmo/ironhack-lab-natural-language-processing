{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display_html\n",
    "display_html(\"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab | Natural Language Processing\n",
    "### SMS: SPAM or HAM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's prepare the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Read Data for the Fraudulent Email Kaggle Challenge\n",
    "- Reduce the training set to speead up development. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read Data for the Fraudulent Email Kaggle Challenge\n",
    "data_train = pd.read_csv(\"data/kg_train.csv\",encoding='latin-1')\n",
    "data_test = pd.read_csv(\"data/kg_test.csv\",encoding='latin-1')\n",
    "\n",
    "# Reduce the training set to speed up development. \n",
    "# Modify for final system\n",
    "print(data_train.shape)\n",
    "data_train.fillna(\"\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's divide the training and test set into two partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_train.loc[:, \"text\"]\n",
    "y_train = data_train.loc[:, \"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "print(string.punctuation)\n",
    "print(stopwords.words(\"english\")[100:110])\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we have to clean the html code removing words\n",
    "\n",
    "- First we remove inline JavaScript/CSS\n",
    "- Then we remove html comments. This has to be done before removing regular tags since comments can contain '>' characters\n",
    "- Next we can remove the remaining tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c85952",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def clean_html_column(series):\n",
    "    import re\n",
    "    # Create a copy of the original column to work with\n",
    "    cleaned_series = series.astype(str).copy()\n",
    "\n",
    "    # 1. Remove inline JavaScript/CSS (script and style tags and their content)\n",
    "    # The regex removes <script>...</script> and <style>...</style> content.\n",
    "    js_css_pattern = r'<(script|style)\\b[^>]*>.*?<\\/\\1>'\n",
    "    cleaned_series = cleaned_series.str.replace(js_css_pattern, '', regex=True, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # 2. Remove HTML comments\n",
    "    # This must be done before general tag removal since comments can contain '>'\n",
    "    comment_pattern = r'<!--.*?-->'\n",
    "    cleaned_series = cleaned_series.str.replace(comment_pattern, '', regex=True)\n",
    "\n",
    "    # 3. Remove all remaining HTML tags\n",
    "    # This is a general, non-greedy pattern to remove any remaining tags.\n",
    "    tag_pattern = r'<[^>]+>'\n",
    "    cleaned_series = cleaned_series.str.replace(tag_pattern, '', regex=True)\n",
    "\n",
    "    return cleaned_series\n",
    "\n",
    "x_train = clean_html_column(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Remove all the special characters\n",
    "    \n",
    "- Remove numbers\n",
    "    \n",
    "- Remove all single characters\n",
    " \n",
    "- Remove single characters from the start\n",
    "\n",
    "- Substitute multiple spaces with single space\n",
    "\n",
    "- Remove prefixed 'b'\n",
    "\n",
    "- Convert to Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d31abe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text_series(text_series: pd.Series) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Performs multiple text cleaning and normalization steps on a Pandas Series:\n",
    "    1. Converts text to lowercase.\n",
    "    2. Removes prefixed 'b' (a common artifact from Python 2 bytes or serialization).\n",
    "    3. Removes special characters.\n",
    "    4. Removes numbers.\n",
    "    5. Removes single characters.\n",
    "    6. Removes single characters from the start of the text.\n",
    "    7. Substitutes multiple spaces with a single space.\n",
    "\n",
    "    Args:\n",
    "        text_series (pd.Series): The Series containing the text to normalize.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A new Series with the normalized text.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Ensure text is string type for .str operations\n",
    "    normalized_series = text_series.astype(str).copy()\n",
    "\n",
    "    # 1. Convert to Lowercase\n",
    "    normalized_series = normalized_series.str.lower()\n",
    "    \n",
    "    # 2. Remove prefixed 'b' (e.g., b'text')\n",
    "    # Using a regex to handle 'b' at the start of the string, optionally with quotes\n",
    "    normalized_series = normalized_series.str.replace(r'^\\s*b[\\'\"]?', '', regex=True)\n",
    "\n",
    "    # 3. Remove all the special characters\n",
    "    # This keeps only alphanumeric characters and spaces\n",
    "    normalized_series = normalized_series.str.replace(r'[^a-z0-9\\s]', ' ', regex=True)\n",
    "    \n",
    "    # 4. Remove numbers (Optional: only remove numbers after special chars are replaced)\n",
    "    normalized_series = normalized_series.str.replace(r'\\d+', ' ', regex=True)\n",
    "\n",
    "    # 5. Remove all single characters (anywhere in the string)\n",
    "    # Matches any single character surrounded by spaces (e.g., ' a ')\n",
    "    normalized_series = normalized_series.str.replace(r'\\s+[a-z]\\s+', ' ', regex=True)\n",
    "    \n",
    "    # 6. Remove single characters from the start\n",
    "    # Matches a single character followed by a space at the start of the string\n",
    "    normalized_series = normalized_series.str.replace(r'^\\s*[a-z]\\s+', ' ', regex=True)\n",
    "    \n",
    "    # 7. Substitute multiple spaces with single space AND trim leading/trailing spaces\n",
    "    # Trim leading/trailing whitespace first\n",
    "    normalized_series = normalized_series.str.strip() \n",
    "    # Replace multiple spaces with single space\n",
    "    normalized_series = normalized_series.str.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "    return normalized_series\n",
    "\n",
    "x_train = normalize_text_series(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Now let's work on removing stopwords\n",
    "Remove the stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "STOPWORDS = stopwords.words(\"english\")\n",
    "def remove_stopwords(text):\n",
    "    words = [word for word in text.split() if word not in STOPWORDS]\n",
    "    return \" \".join(words)\n",
    "\n",
    "x_train = x_train.apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tame Your Text with Lemmatization\n",
    "Break sentences into words, then use lemmatization to reduce them to their base form (e.g., \"running\" becomes \"run\"). See how this creates cleaner data for analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    snowball = SnowballStemmer(\"english\")\n",
    "    words = [snowball.stem(word) for word in text.split()]\n",
    "    return \" \".join(words)\n",
    "\n",
    "x_train = x_train.apply(lemmatize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag Of Words\n",
    "Let's get the 10 top words in ham and spam messages (**EXPLORATORY DATA ANALYSIS**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "ham = x_train[y_train==0]\n",
    "spam = x_train[y_train==1]\n",
    "\n",
    "for message_type, messages in zip((\"spam\", \"ham\"), (spam, ham)):\n",
    "    print(f\"In {message_type} messages the most common 10 words and they frequencies are\")\n",
    "    for word, freq in Counter(\" \".join(list(messages)).split()).most_common(10):\n",
    "        print(f\"{word:10s}\\t{freq}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468540e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add to the original dataframe two additional indicators (money symbols and suspicious words).\n",
    "money_simbol_list = \"|\".join([\"euro\",\"dollar\",\"pound\",\"â‚¬\",r\"\\$\"])\n",
    "suspicious_words = \"|\".join([\"free\",\"cheap\",\"sex\",\"money\",\"account\",\"bank\",\"fund\",\"transfer\",\"transaction\",\"win\",\"deposit\",\"password\"])\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train.name = \"preprocessed_text\"\n",
    "data = pd.DataFrame({\"preprocessed_text\": x_train, \"label\": y_train})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd465dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_val, y_train, y_val = train_test_split(data.drop(columns=\"label\"), data.loc[:, \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train['money_mark'] = data_train['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_train['suspicious_words'] = data_train['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_train['text_len'] = data_train['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_val['money_mark'] = data_val['preprocessed_text'].str.contains(money_simbol_list)*1\n",
    "data_val['suspicious_words'] = data_val['preprocessed_text'].str.contains(suspicious_words)*1\n",
    "data_val['text_len'] = data_val['preprocessed_text'].apply(lambda x: len(x)) \n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How would work the Bag of Words with Count Vectorizer concept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "words = vectorizer.fit_transform(data_train[\"preprocessed_text\"])\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(words.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "- Load the vectorizer\n",
    "\n",
    "- Vectorize all dataset\n",
    "\n",
    "- print the shape of the vetorized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "x_tfidf = tfidf.fit_transform(data[\"preprocessed_text\"])\n",
    "print(x_tfidf.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And the Train a Classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    x_tfidf,\n",
    "    data[\"label\"],\n",
    "    stratify=data[\"label\"],\n",
    ")\n",
    "\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Classification report:\\n{classification_report(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Task - Implement a SPAM/HAM classifier\n",
    "\n",
    "https://www.kaggle.com/t/b384e34013d54d238490103bc3c360ce\n",
    "\n",
    "The classifier can not be changed!!! It must be the MultinimialNB with default parameters!\n",
    "\n",
    "Your task is to **find the most relevant features**.\n",
    "\n",
    "For example, you can test the following options and check which of them performs better:\n",
    "- Using \"Bag of Words\" only\n",
    "- Using \"TF-IDF\" only\n",
    "- Bag of Words + extra flags (money_mark, suspicious_words, text_len)\n",
    "- TF-IDF + extra flags\n",
    "\n",
    "\n",
    "You can work with teams of two persons (recommended)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
